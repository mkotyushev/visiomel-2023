trainer:
  # limit_train_batches: 10
  # limit_val_batches: 10
  max_epochs: 10
  num_sanity_val_steps: 0
  accumulate_grad_batches: 32
  reload_dataloaders_every_n_epochs: 0
  log_every_n_steps: 1
  gradient_clip_val: 20
  check_val_every_n_epoch: 1
  callbacks:
    - class_path: pytorch_lightning.callbacks.LearningRateMonitor
      init_args:
        logging_interval: step
    - class_path: pytorch_lightning.callbacks.ModelCheckpoint
      init_args:
        save_top_k: 1
        monitor: val_ds_ll
        every_n_epochs: 1
        mode: min
        save_on_train_epoch_end: false
        save_last: true
model:
  class_path: src.model.patch_attention_classifier.PatchAttentionClassifier
  init_args:
    grad_checkpointing: true
    patch_embed_backbone_name: swinv2_base_window12to24_192to384_22kft1k
    patch_embed_backbone_ckpt_path: /workspace/visiomel-2023/weights/val_ssup_patches/checkpoints/last.ckpt
    patch_size: 1536
    patch_batch_size: 5
    attention_hidden_dim: 64
    optimizer_init: 
      class_path: torch.optim.AdamW
      init_args:
        weight_decay: 0.01
        eps: 1e-08
        lr: 1e-4
    lr_scheduler_init:
      class_path: src.utils.lr_scheduler.PiecewiceFactorsLRScheduler
      init_args:
        milestones: [0, 0.1, 1.0]
        pieces:
          - class_path: src.utils.lr_scheduler.LinearLRSchedulerPiece
            init_args:
              start_lr: 1e-2
              stop_lr: 1
          - class_path: src.utils.lr_scheduler.CosineLRSchedulerPiece
            init_args:
              start_lr: 1
              stop_lr: 1e-2
    pl_lrs_cfg:
      interval: step
      frequency: 1
    lr_layer_decay: 1.0
    finetuning:
      unfreeze_before_epoch: 500
      unfreeze_layer_names_startswith:
        - pooling
        - classifier
data:
  class_path: src.data.visiomel_datamodule.VisiomelDatamodule
  init_args:
    task: classification
    data_dir_train: /workspace/data/images_page_4_shrink/
    k: 5
    fold_index: 0
    data_shrinked: true
    shrink_preview_scale: null
    # implementing batch_size > 1 is not feasible (patch size is rather large and not many will fit into GPU memory)
    batch_size: 1
    num_workers: 4
    num_workers_saturated: 4
    prefetch_factor: null
    enable_caching: false
    train_resize_type: none
    pin_memory: false
