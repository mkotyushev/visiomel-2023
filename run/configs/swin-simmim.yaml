trainer:
  # limit_train_batches: 10
  # limit_val_batches: 10
  max_epochs: 10
  num_sanity_val_steps: 0
  accumulate_grad_batches: 32
  reload_dataloaders_every_n_epochs: 0
  log_every_n_steps: 1
  gradient_clip_val: 20
  check_val_every_n_epoch: 1
  callbacks:
    - class_path: pytorch_lightning.callbacks.LearningRateMonitor
      init_args:
        logging_interval: step
    - class_path: pytorch_lightning.callbacks.ModelCheckpoint
      init_args:
        save_top_k: 1
        monitor: val_loss_simmim
        every_n_epochs: 1
        mode: min
        save_on_train_epoch_end: false
        save_last: true
model:
  class_path: src.model.swin_transformer_v2_simmim.SwinTransformerV2SimMIM
  init_args:
    grad_checkpointing: true
    model_name: swinv2_large_window12to24_192to384_22kft1k
    pretrained: true
    patch_size: 4
    optimizer_init: 
      class_path: torch.optim.AdamW
      init_args:
        weight_decay: 0.01
        eps: 1e-08
        lr: 1e-4
    lr_scheduler_init:
      class_path: src.utils.lr_scheduler.PiecewiceFactorsLRScheduler
      init_args:
        milestones: [0, 0.1, 1.0]
        pieces:
          - class_path: src.utils.lr_scheduler.LinearLRSchedulerPiece
            init_args:
              start_lr: 1e-2
              stop_lr: 1
          - class_path: src.utils.lr_scheduler.CosineLRSchedulerPiece
            init_args:
              start_lr: 1
              stop_lr: 1e-2
    pl_lrs_cfg:
      interval: step
      frequency: 1
    lr_layer_decay: 0.99
    finetuning: null
data:
  class_path: src.data.visiomel_datamodule.VisiomelTrainDatamoduleSimMIM
  init_args:
    data_dir_train: /workspace/data/images_page_4_shrink/
    img_size: 1536
    data_shrinked: true
    shrink_preview_scale: null
    batch_size: 1
    num_workers: 4
    num_workers_saturated: 4
    enable_caching: true
    train_resize_type: resize
    mask_patch_size: 32
    model_patch_size: 4
    mask_ratio: 0.6
