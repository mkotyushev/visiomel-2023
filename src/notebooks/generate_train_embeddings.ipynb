{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import timm\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "from model.patch_embed_with_backbone import PatchBackbone\n",
    "from data.visiomel_datamodule import VisiomelDatamodule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_index = 4\n",
    "\n",
    "# for val augmented embeddings with 5 repeats\n",
    "task = 'simmim_randaug'\n",
    "train_transform_n_repeats = 5\n",
    "\n",
    "# # for val embeddings w/o augmentations with 1 repeat\n",
    "# task = 'simmim'\n",
    "# train_transform_n_repeats = 1\n",
    "\n",
    "patch_embed_backbone_name = 'swinv2_base_window12to24_192to384_22kft1k'\n",
    "patch_size = 1536\n",
    "patch_embed_backbone_ckpt_path = f'/workspace/visiomel-2023/weights/val_ssup_patches_aug_fold_{fold_index}/checkpoints/epoch=7-step=456.ckpt'\n",
    "patch_batch_size = 2\n",
    "batch_size = 1\n",
    "save_path = f'/workspace/visiomel-2023/weights/val_ssup_patches_aug_fold_{fold_index}/embeddings/'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "datamodule = VisiomelDatamodule(\n",
    "    task=task,\n",
    "    data_dir_train='/workspace/data/images_page_4_shrink/',\t\n",
    "    k=5,\n",
    "    fold_index=fold_index,\n",
    "    data_dir_test=None,\n",
    "    img_size=patch_size,\n",
    "    shrink_preview_scale=None,\n",
    "    batch_size=batch_size,\n",
    "    split_seed=0,\n",
    "    num_workers=4,\n",
    "    num_workers_saturated=4,\n",
    "    pin_memory=False,\n",
    "    prefetch_factor=None,\n",
    "    persistent_workers=True,\n",
    "    sampler=None,\n",
    "    enable_caching=False,\n",
    "    data_shrinked=True,\n",
    "    train_resize_type='none',\n",
    "    train_transform_n_repeats=train_transform_n_repeats,\n",
    ")\n",
    "datamodule.setup()\n",
    "train_dataloader = datamodule.train_dataloader()\n",
    "val_dataloader, _ = datamodule.val_dataloader()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone = timm.create_model(\n",
    "    patch_embed_backbone_name, \n",
    "    img_size=patch_size, \n",
    "    pretrained=False, \n",
    "    num_classes=0\n",
    ")\n",
    "if patch_embed_backbone_ckpt_path is not None:\n",
    "    # If backbone is fine-tuned then it is done via SwinTransformerV2SimMIM\n",
    "    # module, so we need to remove the prefix 'model.encoder.' from the\n",
    "    # checkpoint state_dict keys.\n",
    "    state_dict = {\n",
    "        k \\\n",
    "            .replace('model.encoder.', 'model.'): v \n",
    "        for k, v in \n",
    "        torch.load(patch_embed_backbone_ckpt_path)['state_dict'].items()\n",
    "    }\n",
    "    backbone.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "patch_embed = PatchBackbone(\n",
    "    backbone=backbone, \n",
    "    patch_size=patch_size, \n",
    "    embed_dim=backbone.num_features,\n",
    "    patch_batch_size=patch_batch_size,\n",
    "    patch_embed_caching=False,\n",
    ").cuda().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 268/268 [2:32:31<00:00, 34.15s/it]  \n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    features, labels, paths = [], [], []\n",
    "    for batch in tqdm(val_dataloader):  # batches\n",
    "        x_minibatch, mask_minibatch, y_minibatch, path_minibatch = batch\n",
    "        for x, mask, y, path in zip(x_minibatch, mask_minibatch, y_minibatch, path_minibatch):  # samples\n",
    "            features.append(patch_embed(x.unsqueeze(0).cuda()).detach().cpu())\n",
    "            labels.append(y.detach().cpu())\n",
    "            paths.append(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val = pd.DataFrame({\n",
    "    'path': paths,\n",
    "    'label': labels,\n",
    "    'features': features,\n",
    "})\n",
    "df_val.to_pickle(save_path + 'val_aug.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "visiomel-2023",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
